{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "import struct\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "batch_size = 128\n",
    "n_epochs = 100\n",
    "n_train = 60000\n",
    "n_test = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    #Download and unzip the dataset mnist if it's not already downloaded \n",
    "    #Download from http://yann.lecun.com/exdb/mnist\n",
    "    mnist_folder = \"C:\\\\Users\\\\USX28939\\\\PYTHON_CODE_BASE\\\\GitHub_Doc\\\\stanford-tensorflow-tutorials\\\\examples\\\\data\\\\MNST_DATA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_one_file(download_url, \n",
    "                    local_dest, \n",
    "                    expected_byte=None, \n",
    "                    unzip_and_remove=False):\n",
    "    \"\"\" \n",
    "    Download the file from download_url into local_dest\n",
    "    if the file doesn't already exists.\n",
    "    If expected_byte is provided, check if \n",
    "    the downloaded file has the same number of bytes.\n",
    "    If unzip_and_remove is True, unzip the file and remove the zip file\n",
    "    \"\"\"\n",
    "    if os.path.exists(local_dest) or os.path.exists(local_dest[:-3]):\n",
    "        print('%s already exists' %local_dest)\n",
    "    else:\n",
    "        print('Downloading %s' %download_url)\n",
    "        local_file, headers = urllib.request.urlretrieve(download_url, local_dest)\n",
    "        file_stat = os.stat(local_dest)\n",
    "        if expected_byte:\n",
    "            if file_stat.st_size == expected_byte:\n",
    "                print('Successfully downloaded %s' %local_dest)\n",
    "                if unzip_and_remove:\n",
    "                    with gzip.open(local_dest, 'rb') as f_in, open(local_dest[:-3],'wb') as f_out:\n",
    "                        shutil.copyfileobj(f_in, f_out)\n",
    "                    os.remove(local_dest)\n",
    "            else:\n",
    "                print('The downloaded file has unexpected number of bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-35-d986ab718bac>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-35-d986ab718bac>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    print (urllib.request.urlopen(\"http://yann.lecun.com/exdb/mnist\\\"))\u001b[0m\n\u001b[1;37m                                                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "print (urllib.request.urlopen(\"http://yann.lecun.com/exdb/mnist\\\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_mnist(path):\n",
    "    \"\"\" \n",
    "    Download and unzip the dataset mnist if it's not already downloaded \n",
    "    Download from http://yann.lecun.com/exdb/mnist\n",
    "    \"\"\"\n",
    "    os.mkdir(path)\n",
    "    url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "    filenames = ['train-images-idx3-ubyte.gz',\n",
    "                'train-labels-idx1-ubyte.gz',\n",
    "                't10k-images-idx3-ubyte.gz',\n",
    "                't10k-labels-idx1-ubyte.gz']\n",
    "    expected_bytes = [9912422, 28881, 1648877, 4542]\n",
    "\n",
    "    for filename, byte in zip(filenames, expected_bytes):\n",
    "        download_url = os.path.join(url, filename)\n",
    "        local_dest = os.path.join(path, filename)\n",
    "        print(download_url,\"    -> \", local_dest)\n",
    "        download_one_file(download_url, local_dest, byte, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz     ->  C:\\Users\\USX28939\\PYTHON_CODE_BASE\\GitHub_Doc\\stanford-tensorflow-tutorials\\examples\\data\\MNST_DATA\\train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded C:\\Users\\USX28939\\PYTHON_CODE_BASE\\GitHub_Doc\\stanford-tensorflow-tutorials\\examples\\data\\MNST_DATA\\train-images-idx3-ubyte.gz\n",
      "http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz     ->  C:\\Users\\USX28939\\PYTHON_CODE_BASE\\GitHub_Doc\\stanford-tensorflow-tutorials\\examples\\data\\MNST_DATA\\train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded C:\\Users\\USX28939\\PYTHON_CODE_BASE\\GitHub_Doc\\stanford-tensorflow-tutorials\\examples\\data\\MNST_DATA\\train-labels-idx1-ubyte.gz\n",
      "http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz     ->  C:\\Users\\USX28939\\PYTHON_CODE_BASE\\GitHub_Doc\\stanford-tensorflow-tutorials\\examples\\data\\MNST_DATA\\t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded C:\\Users\\USX28939\\PYTHON_CODE_BASE\\GitHub_Doc\\stanford-tensorflow-tutorials\\examples\\data\\MNST_DATA\\t10k-images-idx3-ubyte.gz\n",
      "http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz     ->  C:\\Users\\USX28939\\PYTHON_CODE_BASE\\GitHub_Doc\\stanford-tensorflow-tutorials\\examples\\data\\MNST_DATA\\t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Successfully downloaded C:\\Users\\USX28939\\PYTHON_CODE_BASE\\GitHub_Doc\\stanford-tensorflow-tutorials\\examples\\data\\MNST_DATA\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "download_mnist(mnist_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_data(path, dataset, flatten):\n",
    "    if dataset != 'train' and dataset != 't10k':\n",
    "        raise NameError('dataset must be train or t10k')\n",
    "\n",
    "    label_file = os.path.join(path, dataset + '-labels-idx1-ubyte')\n",
    "    #print(label_file )\n",
    "    with open(label_file, 'rb') as file:\n",
    "        _, num = struct.unpack(\">II\", file.read(8))\n",
    "        labels = np.fromfile(file, dtype=np.int8) #int8\n",
    "        new_labels = np.zeros((num, 10))\n",
    "        new_labels[np.arange(num), labels] = 1\n",
    "    \n",
    "    img_file = os.path.join(path, dataset + '-images-idx3-ubyte')\n",
    "    with open(img_file, 'rb') as file:\n",
    "        _, num, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "        imgs = np.fromfile(file, dtype=np.uint8).reshape(num, rows, cols) #uint8\n",
    "        imgs = imgs.astype(np.float32) / 255.0\n",
    "        if flatten:\n",
    "            imgs = imgs.reshape([num, -1])\n",
    "\n",
    "    return imgs, new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_mnist(path, flatten=True, num_train=55000):\n",
    "    \"\"\"\n",
    "    Read in the mnist dataset, given that the data is stored in path\n",
    "    Return two tuples of numpy arrays\n",
    "    ((train_imgs, train_labels), (test_imgs, test_labels))\n",
    "    \"\"\"\n",
    "    imgs, labels = parse_data(path, 'train', flatten)\n",
    "    indices = np.random.permutation(labels.shape[0])\n",
    "    train_idx, val_idx = indices[:num_train], indices[num_train:]\n",
    "    train_img, train_labels = imgs[train_idx, :], labels[train_idx, :]\n",
    "    val_img, val_labels = imgs[val_idx, :], labels[val_idx, :]\n",
    "    test = parse_data(path, 't10k', flatten)\n",
    "    return (train_img, train_labels), (val_img, val_labels), test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, val, test = read_mnist(mnist_folder, flatten=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "print(len(val))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices(train)\n",
    "train_data = train_data.shuffle(10000) # if you want to shuffle your data\n",
    "train_data = train_data.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = tf.data.Dataset.from_tensor_slices(test)\n",
    "test_data = test_data.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((?, 784), (?, 10)), types: (tf.float32, tf.float64)>\n"
     ]
    }
   ],
   "source": [
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tf.float32, tf.float64)\n",
      "(TensorShape([Dimension(None), Dimension(784)]), TensorShape([Dimension(None), Dimension(10)]))\n"
     ]
    }
   ],
   "source": [
    "print(train_data.output_types)\n",
    "print(train_data.output_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterator = tf.data.Iterator.from_structure(train_data.output_types,train_data.output_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img, label = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"IteratorGetNext:1\", shape=(?, 10), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_init = iterator.make_initializer(train_data)\n",
    "test_init = iterator.make_initializer(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable Weights already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-a15f1cd3390d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m w = tf.get_variable(name = \"Weights\", dtype = tf.float32, shape = (784,10), initializer = tf.random_normal_initializer(mean=0.0,\n\u001b[1;32m----> 2\u001b[1;33m     stddev=1.0,seed=None,dtype=tf.float32))\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Bias\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0minitializer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m   1201\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1202\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1203\u001b[1;33m       constraint=constraint)\n\u001b[0m\u001b[0;32m   1204\u001b[0m get_variable_or_local_docstring = (\n\u001b[0;32m   1205\u001b[0m     \"\"\"%s\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m   1090\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1092\u001b[1;33m           constraint=constraint)\n\u001b[0m\u001b[0;32m   1093\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1094\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m    423\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 425\u001b[1;33m           constraint=constraint)\n\u001b[0m\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[0;32m    392\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[0;32m    740\u001b[0m                          \u001b[1;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    741\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[1;32m--> 742\u001b[1;33m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[0;32m    743\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable Weights already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n"
     ]
    }
   ],
   "source": [
    "w = tf.get_variable(name = \"Weights\", dtype = tf.float32, shape = (784,10), initializer = tf.random_normal_initializer(mean=0.0,\n",
    "    stddev=1.0,seed=None,dtype=tf.float32))\n",
    "b = tf.get_variable(name = \"Bias\", dtype = tf.float32, shape = (1,1) , initializer = tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = tf.matmul(img,w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entropy = tf.nn.softmax_cross_entropy_with_logits(labels=label, logits=logits)\n",
    "loss = tf.reduce_mean(entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=0.001, momentum = .1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: 12.348731806111891\n",
      "Average loss epoch 1: 10.59124533409296\n",
      "Average loss epoch 2: 9.430858217283737\n",
      "Average loss epoch 3: 8.612977085557095\n",
      "Average loss epoch 4: 7.985617655377055\n",
      "Average loss epoch 5: 7.463224460912305\n",
      "Average loss epoch 6: 7.012512007424998\n",
      "Average loss epoch 7: 6.608266159545543\n",
      "Average loss epoch 8: 6.242408138097718\n",
      "Average loss epoch 9: 5.910529146638027\n",
      "Average loss epoch 10: 5.6061500127925425\n",
      "Average loss epoch 11: 5.326777762590453\n",
      "Average loss epoch 12: 5.0718947865242185\n",
      "Average loss epoch 13: 4.840120219629864\n",
      "Average loss epoch 14: 4.6267435295637265\n",
      "Average loss epoch 15: 4.431375542352366\n",
      "Average loss epoch 16: 4.252670641278112\n",
      "Average loss epoch 17: 4.088293953829034\n",
      "Average loss epoch 18: 3.9375345884367476\n",
      "Average loss epoch 19: 3.798849011021991\n",
      "Average loss epoch 20: 3.6714196848314864\n",
      "Average loss epoch 21: 3.552593929823055\n",
      "Average loss epoch 22: 3.4432861699614414\n",
      "Average loss epoch 23: 3.342257108799247\n",
      "Average loss epoch 24: 3.2493175634118012\n",
      "Average loss epoch 25: 3.1618957034377164\n",
      "Average loss epoch 26: 3.0814826333245566\n",
      "Average loss epoch 27: 3.0059010048245276\n",
      "Average loss epoch 28: 2.9355586869772092\n",
      "Average loss epoch 29: 2.8692185188448707\n",
      "Average loss epoch 30: 2.807147938983385\n",
      "Average loss epoch 31: 2.7490514633267424\n",
      "Average loss epoch 32: 2.694771272914354\n",
      "Average loss epoch 33: 2.641956173541934\n",
      "Average loss epoch 34: 2.5935202878574994\n",
      "Average loss epoch 35: 2.546114222947941\n",
      "Average loss epoch 36: 2.5026873280835704\n",
      "Average loss epoch 37: 2.4605980379636896\n",
      "Average loss epoch 38: 2.4201232480448347\n",
      "Average loss epoch 39: 2.382348555742308\n",
      "Average loss epoch 40: 2.3458014285841653\n",
      "Average loss epoch 41: 2.311197392053382\n",
      "Average loss epoch 42: 2.2787907791692157\n",
      "Average loss epoch 43: 2.245576035144717\n",
      "Average loss epoch 44: 2.2156511750332144\n",
      "Average loss epoch 45: 2.186068594178488\n",
      "Average loss epoch 46: 2.157944498505703\n",
      "Average loss epoch 47: 2.130812099645304\n",
      "Average loss epoch 48: 2.1047017834907353\n",
      "Average loss epoch 49: 2.079948034674622\n",
      "Average loss epoch 50: 2.055891585350037\n",
      "Average loss epoch 51: 2.0321431448293286\n",
      "Average loss epoch 52: 2.0093379034552465\n",
      "Average loss epoch 53: 1.9879068998403326\n",
      "Average loss epoch 54: 1.966865416460259\n",
      "Average loss epoch 55: 1.945923227348993\n",
      "Average loss epoch 56: 1.926761802684429\n",
      "Average loss epoch 57: 1.9071119993232017\n",
      "Average loss epoch 58: 1.8890882941179497\n",
      "Average loss epoch 59: 1.870598472273627\n",
      "Average loss epoch 60: 1.8529537954995798\n",
      "Average loss epoch 61: 1.8358547731887462\n",
      "Average loss epoch 62: 1.819505798816681\n",
      "Average loss epoch 63: 1.8034743810808935\n",
      "Average loss epoch 64: 1.7873354074566863\n",
      "Average loss epoch 65: 1.7720918486284656\n",
      "Average loss epoch 66: 1.757430872251821\n",
      "Average loss epoch 67: 1.7429501853709997\n",
      "Average loss epoch 68: 1.7288276134535323\n",
      "Average loss epoch 69: 1.7157890821612158\n",
      "Average loss epoch 70: 1.7021145682002223\n",
      "Average loss epoch 71: 1.689080465671628\n",
      "Average loss epoch 72: 1.676615153634271\n",
      "Average loss epoch 73: 1.6637838719889175\n",
      "Average loss epoch 74: 1.6518330872058868\n",
      "Average loss epoch 75: 1.640743539776913\n",
      "Average loss epoch 76: 1.6290185170118199\n",
      "Average loss epoch 77: 1.6169152821219244\n",
      "Average loss epoch 78: 1.6061839641526687\n",
      "Average loss epoch 79: 1.5958566403666208\n",
      "Average loss epoch 80: 1.5848537590614584\n",
      "Average loss epoch 81: 1.5745469249958215\n",
      "Average loss epoch 82: 1.5645813926707866\n",
      "Average loss epoch 83: 1.5545769396216371\n",
      "Average loss epoch 84: 1.5453257567660754\n",
      "Average loss epoch 85: 1.5354432908601539\n",
      "Average loss epoch 86: 1.5267583486645722\n",
      "Average loss epoch 87: 1.517514146205991\n",
      "Average loss epoch 88: 1.5080706202706626\n",
      "Average loss epoch 89: 1.4995552287545315\n",
      "Average loss epoch 90: 1.4908329068228257\n",
      "Average loss epoch 91: 1.4822020008120427\n",
      "Average loss epoch 92: 1.4743522291959719\n",
      "Average loss epoch 93: 1.4663322537444359\n",
      "Average loss epoch 94: 1.4578773572001347\n",
      "Average loss epoch 95: 1.450708946377732\n",
      "Average loss epoch 96: 1.44313884363618\n",
      "Average loss epoch 97: 1.4353334324304448\n",
      "Average loss epoch 98: 1.4279488441555999\n",
      "Average loss epoch 99: 1.420595547626185\n",
      "Total time: 69.74832820892334 seconds\n",
      "Accuracy 0.7408\n"
     ]
    }
   ],
   "source": [
    "preds = tf.nn.softmax(logits)\n",
    "correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(label, 1))\n",
    "accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "\n",
    "writer = tf.summary.FileWriter('C:/Users/USX28939/PYTHON_CODE_BASE/graphs', tf.get_default_graph())\n",
    "# to access the tensorboard graph use tensorboard --logdir training:C:\\path of the log dir\n",
    "# there is no equal sign after the logdir and add any text before the drive as its a bug\n",
    "with tf.Session() as sess:\n",
    "   \n",
    "    start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # train the model n_epochs times\n",
    "    for i in range(n_epochs): \t\n",
    "        sess.run(train_init)\t# drawing samples from train_data\n",
    "        total_loss = 0\n",
    "        n_batches = 0\n",
    "        try:\n",
    "            while True:\n",
    "                _, l = sess.run([optimizer, loss])\n",
    "                total_loss += l\n",
    "                n_batches += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "    print('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "\n",
    "    # test the model\n",
    "    sess.run(test_init)\t\t\t# drawing samples from test_data\n",
    "    total_correct_preds = 0\n",
    "    try:\n",
    "        while True:\n",
    "            accuracy_batch = sess.run(accuracy)\n",
    "            total_correct_preds += accuracy_batch\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass\n",
    "\n",
    "    print('Accuracy {0}'.format(total_correct_preds/n_test))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os as os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\USX28939\\\\PYTHON_CODE_BASE\\\\GitHub_Doc\\\\TensorFlow_Framework'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
