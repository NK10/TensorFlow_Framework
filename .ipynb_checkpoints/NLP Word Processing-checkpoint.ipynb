{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"The striped bats are hanging on their feet for best\",\n",
    "        \"This is testing the Natural language processing\",\n",
    "        \"Natural processing learning is great and best\",\n",
    "        \"There are different type of language in the world\",\n",
    "        \"processing is doing something on feet\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StopWord Removal Methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gensim solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords, STOPWORDS \n",
    "#add custom words for gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The striped bats hanging feet best', 'This testing Natural language processing', 'Natural leaning great best', 'different type languages world', 'processing feet']\n"
     ]
    }
   ],
   "source": [
    "temp = []\n",
    "for i in text:\n",
    "    token= remove_stopwords(i)\n",
    "    temp.append(token)\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Custom Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = STOPWORDS.union(set(['world', 'feet']))\n",
    "#my_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final temp is  [['The', 'striped', 'bats', 'hanging', 'best'], ['This', 'testing', 'Natural', 'language', 'processing'], ['Natural', 'leaning', 'great', 'best'], ['different', 'type', 'languages'], ['processing']]\n"
     ]
    }
   ],
   "source": [
    "temp = []\n",
    "for i in text:\n",
    "    st= i.split(\" \")\n",
    "    token= [\"\".join(st[j]) for j in range(len(st)) if st[j] not in my_stop_words]\n",
    "    temp.append(token)\n",
    "print(\"final temp is \", temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "spacy_nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print('Number of stop words: %d' % len(spacy_stopwords))\n",
    "print('First ten stop words: %s' % list(spacy_stopwords)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = []\n",
    "for i in text:\n",
    "    doc = spacy_nlp(i)\n",
    "    tokens = [token.text for token in doc if not token.is_stop]\n",
    "    final_list.append(tokens)\n",
    "print(final_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add customize stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = []\n",
    "customize_stop_words = ['world', 'feet']\n",
    "for w in customize_stop_words:\n",
    "    spacy_nlp.vocab[w].is_stop = True\n",
    "for i in text:\n",
    "    doc = spacy_nlp(i)\n",
    "    tokens = [token.text for token in doc if not token.is_stop]\n",
    "    final_list.append(tokens)\n",
    "print(final_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk_stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stopwords.append(\"processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'striped', 'bats', 'hanging', 'feet', 'best'], ['This', 'testing', 'Natural', 'language'], ['Natural', 'learning', 'great', 'best'], ['There', 'different', 'type', 'language', 'world'], ['Processing', 'something', 'feet']]\n"
     ]
    }
   ],
   "source": [
    "temp= []\n",
    "for i in text:\n",
    "    tokens = nltk.tokenize.word_tokenize(i)\n",
    "    tokens = [token for token in tokens if not token in nltk_stopwords]\n",
    "    temp.append(tokens)\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lower Case Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the striped bats are hanging on their feet for best\n",
      "this is testing the natural language processing\n",
      "natural leaning is great and best\n",
      "there are different type of languages in the world\n",
      "processing is doing something on feet\n"
     ]
    }
   ],
   "source": [
    "for i in text:\n",
    "    print(i.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import lemmatize\n",
    "import pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gensim.utils.lemmatize(content, allowed_tags=<_sre.SRE_Pattern object>, light=False, stopwords=frozenset([]), \n",
    "                       min_length=2, max_length=15)  \n",
    "Parameters:\t  \n",
    "content (str) – Input string  \n",
    "allowed_tags (_sre.SRE_Pattern, optional) – Compiled regexp to select POS that will be used. Only considers nouns,   verbs, adjectives and adverbs by default (=all other lemmas are discarded).  \n",
    "light (bool, optional) – DEPRECATED FLAG, DOESN’T SUPPORT BY pattern.  \n",
    "stopwords (frozenset, optional) – Set of words that will be removed from output.  \n",
    "min_length (int, optional) – Minimal token length in output (inclusive).  \n",
    "max_length (int, optional) – Maximal token length in output (inclusive).  \n",
    "only support following :    \n",
    "JJ\tAdjective  \n",
    "NN\tNoun, singular or mass  \n",
    "VB\tVerb, base form  \n",
    "RB\tAdverb  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'striped/JJ', b'bat/NN', b'be/VB', b'hang/VB', b'foot/NN', b'best/JJ'], [b'be/VB', b'test/VB', b'natural/JJ', b'language/NN', b'processing/NN'], [b'natural/JJ', b'lean/VB', b'be/VB', b'great/JJ', b'best/VB'], [b'be/VB', b'different/JJ', b'type/NN', b'language/NN', b'world/NN'], [b'processing/NN', b'be/VB', b'do/VB', b'something/NN', b'foot/NN']]\n"
     ]
    }
   ],
   "source": [
    "# to use lemmatize, we need to install pip install pattern3\n",
    "'''Pattern is Web mining module for Python, with tools for scraping, natural language\n",
    "processing, machine learning, network analysis and visualization.'''\n",
    "temp = []\n",
    "for i in text:\n",
    "    token = lemmatize(i)\n",
    "    temp.append(token)\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordnet is a large, freely and publicly available lexical database for the English language aiming to establish structured semantic relationships between words. It offers lemmatization capabilities as well and is one of the earliest and most commonly used lemmatizers.\n",
    "\n",
    "NLTK offers an interface to it, but you have to download it first in order to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'striped', 'bat', 'are', 'hanging', 'on', 'their', 'foot', 'for', 'best'], ['This', 'is', 'testing', 'the', 'Natural', 'language', 'processing'], ['Natural', 'leaning', 'is', 'great', 'and', 'best'], ['there', 'are', 'different', 'type', 'of', 'language', 'in', 'the', 'world'], ['processing', 'is', 'doing', 'something', 'on', 'foot']]\n"
     ]
    }
   ],
   "source": [
    "#You can also get better results if you first do POS tagging and then provide the POS information to the lemmatizer.\n",
    "#You must lemmatize each word separately. Instead, you lemmatize sentences. \n",
    "#https://medium.com/@gaurav5430/using-nltk-for-lemmatizing-sentences-c1bfff963258\n",
    "temp = []\n",
    "for i in text:\n",
    "    token = [\"\".join(lemmatizer.lemmatize(word)) for word in word_tokenize(i)]\n",
    "    temp.append(token)\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "spacy_nlp = en_core_web_sm.load(disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the stripe bat be hang on -PRON- foot for good', 'this be test the natural language processing', 'natural leaning be great and good', 'there be different type of language in the world', 'processing be do something on foot']\n"
     ]
    }
   ],
   "source": [
    "final_list = []\n",
    "for i in text:\n",
    "    doc = spacy_nlp(i)\n",
    "    tokens = \" \".join([token.lemma_ for token in doc])\n",
    "    final_list.append(tokens)\n",
    "print(final_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gensim solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the stripe bat ar hang on their feet for best',\n",
       " 'thi is test the natur languag process',\n",
       " 'natur lean is great and best',\n",
       " 'there ar differ type of languag in the world',\n",
       " 'process is do someth on feet']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = []\n",
    "p = PorterStemmer()\n",
    "p.stem_documents(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'stripe', 'bat', 'are', 'hang', 'on', 'their', 'feet', 'for', 'best'], ['thi', 'is', 'test', 'the', 'natur', 'languag', 'process'], ['natur', 'learn', 'is', 'great', 'and', 'best'], ['there', 'are', 'differ', 'type', 'of', 'languag', 'in', 'the', 'world'], ['process', 'is', 'do', 'someth', 'on', 'feet']]\n"
     ]
    }
   ],
   "source": [
    "temp= []\n",
    "pp = PorterStemmer()\n",
    "for i in text:\n",
    "    token = [\"\".join(pp.stem(w)) for w in word_tokenize(i)]\n",
    "    temp.append(token)\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocab Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer(num_words= 10) #numwords tell the size of the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit_on_texts Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency. So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 1; word_index[\"cat\"] = 2 it is word -> index dictionary so every word gets a unique integer value. 0 is reserved for padding. So lower integer means more frequent word (often the first few are stop words because they appear a lot).  \n",
    "  \n",
    "texts_to_sequences Transforms each text in texts to a sequence of integers. So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary. Nothing more, nothing less, certainly no magic involved.  \n",
    "  \n",
    "Why don't combine them? Because you almost always fit once and convert to sequences many times. You will fit on your training corpus once and use that exact same word_index dictionary at train / eval / testing / prediction time to convert actual text into sequences to feed them to the network. So it makes sense to keep those methods separate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3, 4, 5, 6], [2, 1, 7, 8, 9], [7, 2, 6], [3, 8, 1], [9, 2, 4, 5]] {'the': 1, 'is': 2, 'are': 3, 'on': 4, 'feet': 5, 'best': 6, 'natural': 7, 'language': 8, 'processing': 9, 'striped': 10, 'bats': 11, 'hanging': 12, 'their': 13, 'for': 14, 'this': 15, 'testing': 16, 'learning': 17, 'great': 18, 'and': 19, 'there': 20, 'different': 21, 'type': 22, 'of': 23, 'in': 24, 'world': 25, 'doing': 26, 'something': 27}\n"
     ]
    }
   ],
   "source": [
    "tok.fit_on_texts(text)\n",
    "t = tok.texts_to_sequences(text)\n",
    "print(t, tok.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'is': 2,\n",
       " 'are': 3,\n",
       " 'on': 4,\n",
       " 'feet': 5,\n",
       " 'best': 6,\n",
       " 'natural': 7,\n",
       " 'language': 8,\n",
       " 'processing': 9,\n",
       " 'striped': 10,\n",
       " 'bats': 11,\n",
       " 'hanging': 12,\n",
       " 'their': 13,\n",
       " 'for': 14,\n",
       " 'this': 15,\n",
       " 'testing': 16,\n",
       " 'learning': 17,\n",
       " 'great': 18,\n",
       " 'and': 19,\n",
       " 'there': 20,\n",
       " 'different': 21,\n",
       " 'type': 22,\n",
       " 'of': 23,\n",
       " 'in': 24,\n",
       " 'world': 25,\n",
       " 'doing': 26,\n",
       " 'something': 27}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF and CountVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The striped bats are hanging on their feet for best',\n",
       " 'This is testing the Natural language processing',\n",
       " 'Natural learning is great and best',\n",
       " 'There are different type of language in the world',\n",
       " 'Processing is doing something on feet']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "count=  CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "table  =tfidf.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.30410743, 0.36635462, 0.        , 0.36635462,\n",
       "        0.        , 0.        , 0.36635462, 0.        , 0.        ,\n",
       "        0.45408711, 0.30410743, 0.        , 0.        , 0.45408711,\n",
       "        0.        , 0.        ]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.toarray()[1:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Countvector counts the numbers of words in a record and not the overall count of the word in documents.\n",
    "for example if the text is :  \n",
    "corpus = [  \n",
    "'This is the first document.',  \n",
    "'This document is the second document.',  \n",
    "'And this is the third one.',  \n",
    "'Is this the first document?'  \n",
    " the output is :  \n",
    " [[0 1 1 1 0 0 1 0 1]  \n",
    " [0 2 0 1 0 1 1 0 1]  \n",
    " [1 0 0 1 1 0 1 1 1]  \n",
    " [0 1 1 1 0 0 1 0 1]]  \n",
    " look for the 2nd row where the document appears twice in the record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = count.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "        0, 0, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co.toarray()[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(set(),\n",
       " {'the': 21,\n",
       "  'striped': 19,\n",
       "  'bats': 2,\n",
       "  'are': 1,\n",
       "  'hanging': 9,\n",
       "  'on': 16,\n",
       "  'their': 22,\n",
       "  'feet': 6,\n",
       "  'for': 7,\n",
       "  'best': 3,\n",
       "  'this': 24,\n",
       "  'is': 11,\n",
       "  'testing': 20,\n",
       "  'natural': 14,\n",
       "  'language': 12,\n",
       "  'processing': 17,\n",
       "  'learning': 13,\n",
       "  'great': 8,\n",
       "  'and': 0,\n",
       "  'there': 23,\n",
       "  'different': 4,\n",
       "  'type': 25,\n",
       "  'of': 15,\n",
       "  'in': 10,\n",
       "  'world': 26,\n",
       "  'doing': 5,\n",
       "  'something': 18})"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count.stop_words_ , count.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 27)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.toarray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.machinelearningplus.com/nlp/cosine-similarity/\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the documents\n",
    "doc_trump = \"Mr. Trump became president after winning the political election. Though he lost the support of some republican friends, Trump is friends with President Putin\"\n",
    "doc_election = \"President Trump says Putin had no political interference is the election outcome. He says it was a witchhunt by political parties. He claimed President Putin is a friend who had nothing to do with the election\"\n",
    "doc_putin = \"Post elections, Vladimir Putin became President of Russia. President Putin had served as the Prime Minister earlier in his political career\"\n",
    "\n",
    "documents = [doc_trump, doc_election, doc_putin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>career</th>\n",
       "      <th>claimed</th>\n",
       "      <th>earlier</th>\n",
       "      <th>election</th>\n",
       "      <th>elections</th>\n",
       "      <th>friend</th>\n",
       "      <th>friends</th>\n",
       "      <th>interference</th>\n",
       "      <th>lost</th>\n",
       "      <th>minister</th>\n",
       "      <th>mr</th>\n",
       "      <th>outcome</th>\n",
       "      <th>parties</th>\n",
       "      <th>political</th>\n",
       "      <th>post</th>\n",
       "      <th>president</th>\n",
       "      <th>prime</th>\n",
       "      <th>putin</th>\n",
       "      <th>republican</th>\n",
       "      <th>russia</th>\n",
       "      <th>says</th>\n",
       "      <th>served</th>\n",
       "      <th>support</th>\n",
       "      <th>trump</th>\n",
       "      <th>vladimir</th>\n",
       "      <th>winning</th>\n",
       "      <th>witchhunt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc_trump</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_election</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_putin</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              career  claimed  earlier  election  elections  ...  support  trump  vladimir  winning  witchhunt\n",
       "doc_trump          0        0        0         1          0  ...        1      2         0        1          0\n",
       "doc_election       0        1        0         2          0  ...        0      1         0        0          1\n",
       "doc_putin          1        0        1         0          1  ...        0      0         1        0          0\n",
       "\n",
       "[3 rows x 27 columns]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Document Term Matrix\n",
    "import pandas as pd\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "sparse_matrix = count_vectorizer.fit_transform(documents)\n",
    "# OPTIONAL: Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\n",
    "doc_term_matrix = sparse_matrix.todense()\n",
    "df = pd.DataFrame(doc_term_matrix, \n",
    "                  columns=count_vectorizer.get_feature_names(), \n",
    "                  index=['doc_trump', 'doc_election', 'doc_putin'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>career</th>\n",
       "      <th>claimed</th>\n",
       "      <th>earlier</th>\n",
       "      <th>election</th>\n",
       "      <th>elections</th>\n",
       "      <th>friend</th>\n",
       "      <th>friends</th>\n",
       "      <th>interference</th>\n",
       "      <th>lost</th>\n",
       "      <th>minister</th>\n",
       "      <th>mr</th>\n",
       "      <th>outcome</th>\n",
       "      <th>parties</th>\n",
       "      <th>political</th>\n",
       "      <th>post</th>\n",
       "      <th>president</th>\n",
       "      <th>prime</th>\n",
       "      <th>putin</th>\n",
       "      <th>republican</th>\n",
       "      <th>russia</th>\n",
       "      <th>says</th>\n",
       "      <th>served</th>\n",
       "      <th>support</th>\n",
       "      <th>trump</th>\n",
       "      <th>vladimir</th>\n",
       "      <th>winning</th>\n",
       "      <th>witchhunt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc_trump</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.203368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.53481</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267405</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267405</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157934</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315867</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157934</td>\n",
       "      <td>0.267405</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267405</td>\n",
       "      <td>0.406737</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267405</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_election</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.241982</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.368067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.241982</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.241982</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.241982</td>\n",
       "      <td>0.241982</td>\n",
       "      <td>0.285837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.483963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184033</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.241982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_putin</th>\n",
       "      <td>0.287012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.169514</td>\n",
       "      <td>0.287012</td>\n",
       "      <td>0.339028</td>\n",
       "      <td>0.287012</td>\n",
       "      <td>0.339028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                career   claimed   earlier  election  ...     trump  vladimir   winning  witchhunt\n",
       "doc_trump     0.000000  0.000000  0.000000  0.203368  ...  0.406737  0.000000  0.267405   0.000000\n",
       "doc_election  0.000000  0.241982  0.000000  0.368067  ...  0.184033  0.000000  0.000000   0.241982\n",
       "doc_putin     0.287012  0.000000  0.287012  0.000000  ...  0.000000  0.287012  0.000000   0.000000\n",
       "\n",
       "[3 rows x 27 columns]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Document Term Matrix\n",
    "\n",
    "#could have used the TfidfVectorizer() instead of CountVectorizer(), \n",
    "#because it would have downweighted words that occur frequently across docuemnts.\n",
    "\n",
    "count_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "sparse_matrix = count_vectorizer.fit_transform(documents)\n",
    "# OPTIONAL: Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\n",
    "doc_term_matrix = sparse_matrix.todense()\n",
    "df = pd.DataFrame(doc_term_matrix, \n",
    "                  columns=count_vectorizer.get_feature_names(), \n",
    "                  index=['doc_trump', 'doc_election', 'doc_putin'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity,cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.33027897 0.18740386]\n",
      " [0.33027897 1.         0.24226661]\n",
      " [0.18740386 0.24226661 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity(df, df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.66972103 0.81259614]\n",
      " [0.66972103 0.         0.75773339]\n",
      " [0.81259614 0.75773339 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(cosine_distances(df,df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need to consider the semantic meaning should be considered. That is, words similar in meaning should be treated as similar. For Example, ‘President’ vs ‘Prime minister’, ‘Food’ vs ‘Dish’, ‘Hi’ vs ‘Hello’ should be considered similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the documents\n",
    "doc_soup = \"Soup is a primarily liquid food, generally served warm or hot (but may be cool or cold), that is made by combining ingredients of meat or vegetables with stock, juice, water, or another liquid. \"\n",
    "doc_noodles = \"Noodles are a staple food in many cultures. They are made from unleavened dough which is stretched, extruded, or rolled flat and cut into one of a variety of shapes.\"\n",
    "doc_dosa = \"Dosa is a type of pancake from the Indian subcontinent, made from a fermented batter. It is somewhat similar to a crepe in appearance. Its main ingredients are rice and black gram.\"\n",
    "documents = [doc_trump, doc_election, doc_putin, doc_soup, doc_noodles, doc_dosa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.0\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "# upgrade gensim if you can't import softcossim\n",
    "from gensim.matutils import softcossim \n",
    "from gensim import corpora\n",
    "import gensim.downloader as api\n",
    "from gensim.utils import simple_preprocess\n",
    "print(gensim.__version__)\n",
    "# Download the FastText model\n",
    "fasttext_model300 = api.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "# Prepare a dictionary and a corpus.\n",
    "dictionary = corpora.Dictionary([simple_preprocess(doc) for doc in documents])\n",
    "\n",
    "# Prepare the similarity matrix\n",
    "similarity_matrix = fasttext_model300.similarity_matrix(dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)\n",
    "\n",
    "# Convert the sentences into bag-of-words vectors.\n",
    "sent_1 = dictionary.doc2bow(simple_preprocess(doc_trump))\n",
    "sent_2 = dictionary.doc2bow(simple_preprocess(doc_election))\n",
    "sent_3 = dictionary.doc2bow(simple_preprocess(doc_putin))\n",
    "sent_4 = dictionary.doc2bow(simple_preprocess(doc_soup))\n",
    "sent_5 = dictionary.doc2bow(simple_preprocess(doc_noodles))\n",
    "sent_6 = dictionary.doc2bow(simple_preprocess(doc_dosa))\n",
    "\n",
    "sentences = [sent_1, sent_2, sent_3, sent_4, sent_5, sent_6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5842470143211804\n"
     ]
    }
   ],
   "source": [
    "# Compute soft cosine similarity\n",
    "print(softcossim(sent_1, sent_2, similarity_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_trump</th>\n",
       "      <th>doc_election</th>\n",
       "      <th>doc_putin</th>\n",
       "      <th>doc_soup</th>\n",
       "      <th>doc_noodles</th>\n",
       "      <th>doc_dosa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc_trump</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_election</th>\n",
       "      <td>0.58</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_putin</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_soup</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_noodles</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_dosa</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              doc_trump  doc_election  doc_putin  doc_soup  doc_noodles  doc_dosa\n",
       "doc_trump          1.00          0.58       0.56      0.28         0.34      0.40\n",
       "doc_election       0.58          1.00       0.54      0.25         0.31      0.43\n",
       "doc_putin          0.56          0.54       1.00      0.19         0.25      0.36\n",
       "doc_soup           0.28          0.25       0.19      1.00         0.50      0.38\n",
       "doc_noodles        0.34          0.31       0.25      0.50         1.00      0.56\n",
       "doc_dosa           0.40          0.43       0.36      0.38         0.56      1.00"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col  = [\"doc_trump\",\"doc_election\",\"doc_putin\",\"doc_soup\",\"doc_noodles\",\"doc_dosa\"]\n",
    "\n",
    "def create_soft_cossim_matrix(sentences):\n",
    "    len_array = np.arange(len(sentences))\n",
    "    xx, yy = np.meshgrid(len_array, len_array)\n",
    "    cossim_mat = pd.DataFrame([[round(softcossim(sentences[i],sentences[j], similarity_matrix) ,2) for i, j in zip(x,y)] for y, x in zip(xx, yy)],\n",
    "                             columns=col, index = col )\n",
    "    return cossim_mat\n",
    "\n",
    "create_soft_cossim_matrix(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['stripe', 'bat', 'hang', 'feet', 'best'], ['test', 'natur', 'languag', 'process'], ['natur', 'learn', 'great', 'best'], ['differ', 'type', 'languag', 'world'], ['process', 'feet']]\n"
     ]
    }
   ],
   "source": [
    "# to automatically process the text by applying the following default filters\n",
    "'''DEFAULT_FILTERS = [lambda x: x.lower(), strip_tags, strip_punctuation, strip_multiple_whitespaces, \n",
    "                   strip_numeric, remove_stopwords, strip_short (any word len less than 3, stem_text]'''\n",
    "from gensim.parsing.preprocessing import preprocess_documents\n",
    "t = preprocess_documents(text,)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import PorterStemmer, STOPWORDS, remove_stopwords\n",
    "from gensim.utils import lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "nltk_stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
